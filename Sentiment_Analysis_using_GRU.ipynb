{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device(type='cuda')\n",
    "else:\n",
    "    device=torch.device(type='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe'>\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.IMDB(split='train')\n",
    "eval_data=datasets.IMDB(split='test')\n",
    "\n",
    "print(type(train_data))  #type of train_data and test_data is iterdatapipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.functional.to_map_style_dataset.<locals>._MapStyleDataset'>\n"
     ]
    }
   ],
   "source": [
    "train_map_data=to_map_style_dataset(train_data) \n",
    "#for sentiment analysis we want label and review so we are converting them into mapstyledataset\n",
    "#this train_map_data will have list of tuple and tuple will have label and review means pair of int and str \n",
    "print(type(train_map_data))\n",
    "test_map_data=to_map_style_dataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=get_tokenizer('basic_english',language='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocab\n",
    "\n",
    "def build_vocab(train_map_data,tokenizer):\n",
    "    reviews=[]           #for creating vocab we don't want labels\n",
    "    for label,review in train_map_data:\n",
    "      reviews.append(review)\n",
    "    vocab=build_vocab_from_iterator(\n",
    "        map(tokenizer,reviews),\n",
    "        min_freq=2,\n",
    "        specials=[\"<unk>\",\"<eos>\",\"<pad>\"]\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=build_vocab(train_map_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51719\n"
     ]
    }
   ],
   "source": [
    "vocab_size=vocab.__len__()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=300\n",
    "max_norm=1\n",
    "embed_dim=300\n",
    "batch_size=32\n",
    "text_pipeline = lambda x: vocab(tokenizer(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data(batch, text_pipeline):\n",
    "     reviews, targets = [], [] \n",
    "     pad=2\n",
    "     eos=1\n",
    "     for label,review in batch:\n",
    "        \n",
    "         review_tokens_ids = text_pipeline(review)\n",
    "         review_tokens_ids = review_tokens_ids[:max_seq_len]\n",
    "         review_tokens_ids.append(eos)\n",
    "         l=len(review_tokens_ids)\n",
    "         x=[pad]*301\n",
    "         x[:l]=review_tokens_ids\n",
    "         reviews.append(x)\n",
    "         targets.append(label)\n",
    "     reviews = torch.tensor(reviews, dtype=torch.long)\n",
    "     targets = torch.tensor(targets, dtype=torch.long)\n",
    "     \n",
    "     return reviews, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating datasets\n",
    "traindl=DataLoader(\n",
    "  train_map_data,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True,\n",
    "  collate_fn=partial(collate_data,text_pipeline=text_pipeline)\n",
    ")\n",
    "evaldl=DataLoader(\n",
    "  test_map_data,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False,\n",
    "  collate_fn=partial(collate_data,text_pipeline=text_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture of GRU and Neural Network\n",
    "\n",
    "class SNN(nn.Module):\n",
    "  def __init__(self, input_size, embed_size, hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding=nn.Embedding(input_size,embed_size) # here input_dim=max_seq_len*batch_size and emedding_size\n",
    "    self.rnn=nn.GRU(embed_size,hidden_size,batch_first=True)  #hidden_dim how many recurrent neurons in one GRU\n",
    "    self.dropout=nn.Dropout(0.2) #for preventing overfitting\n",
    "    self.out=nn.Linear(in_features=hidden_size,out_features=2)  #for sentiment analysis output will be positive and negative so for binary  classification we have 2 neurons\n",
    "\n",
    "\n",
    "  def forward(self,x): # x=(batch_size*max_seq_len)\n",
    "    x=self.embedding(x) # embedding layer\n",
    "    #after embedding x=(batch_size*max_seq_len*embed_size)\n",
    "    x=self.dropout(x) # dropout layer\n",
    "    outputs, hidden=self.rnn(x) # GRU layer\n",
    "    # GRU gives two outputs one is all the outputs of final GRU layers which ha dimension =(batch_size*D*max_seq_len*h_out) hout=hidden_size and D= w for bidirectional GRU * num_layer of GRU \n",
    "    #second output is final hidden state of GRU whichi will have dimension of (batch_size*D*h_out)\n",
    "    hidden.squeeze_(0) #now, batch_size x hidden_size \n",
    "    logits=self.out(hidden)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=300\n",
    "hidden_size=100\n",
    "\n",
    "sentinn=SNN(vocab_size,embed_size,hidden_size).to(device) \n",
    "loss_fn=nn.CrossEntropyLoss(ignore_index=2).to(device)\n",
    "lr=0.001\n",
    "opt=optim.Adam(params=sentinn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    sentinn.train()\n",
    "    track_loss = 0\n",
    "    num_corrects = 0\n",
    "    total_samples = 0\n",
    "    for i, (reviews_ids, sentiments) in enumerate(traindl):\n",
    "        reviews_ids = reviews_ids.to(device)\n",
    "        sentiments = (sentiments.to(device)) - 1   #because review will have 2 labels 1 and 2 so coverting into 0 and 1\n",
    "        logits = sentinn(reviews_ids)\n",
    "        # print(f\"logits shape: {logits.shape}, sentiments shape: {sentiments.shape}\")\n",
    "        loss = loss_fn(logits, sentiments)\n",
    "        track_loss += loss.item()\n",
    "        num_corrects += (torch.argmax(logits, dim=1) == sentiments).type(torch.float).sum().item()\n",
    "        total_samples += reviews_ids.shape[0]\n",
    "        running_loss = track_loss / (i + 1)  # Average loss over batches\n",
    "        running_acc = (num_corrects / total_samples) * 100  # Accuracy in percentage\n",
    "        print(\"Running Loss is\",running_loss)\n",
    "        print(\"Running Accuracy is\",running_acc)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    epoch_loss = running_loss\n",
    "    epoch_acc = running_acc\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def eval_one_epoch():\n",
    "    sentinn.eval()\n",
    "    track_loss = 0\n",
    "    num_corrects = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for i, (reviews_ids, sentiments) in enumerate(evaldl):\n",
    "            reviews_ids = reviews_ids.to(device)\n",
    "            sentiments = sentiments.to(device) - 1\n",
    "            logits = sentinn(reviews_ids)\n",
    "            loss = loss_fn(logits, sentiments)\n",
    "            track_loss += loss.item()\n",
    "            num_corrects += (torch.argmax(logits, dim=1) == sentiments).type(torch.float).sum().item()\n",
    "            total_samples += reviews_ids.shape[0]\n",
    "            running_loss = track_loss / (i + 1)  # Average loss over batches\n",
    "            running_acc = (num_corrects / total_samples) * 100  # Accuracy in percentage\n",
    "            print(\"Running Loss is\",running_loss)\n",
    "            print(\"Running Accuracy is\",running_acc)\n",
    "    epoch_loss = running_loss\n",
    "    epoch_acc = running_acc\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1, Running Loss is 0.6803618669509888\n",
      "Running Accuracy is 56.25\n",
      "Running Loss is 0.6928572058677673\n",
      "Running Accuracy is 48.4375\n",
      "Running Loss is 0.696658194065094\n",
      "Running Accuracy is 45.83333333333333\n",
      "Running Loss is 0.6981592625379562\n",
      "Running Accuracy is 46.875\n",
      "Running Loss is 0.6992671251296997\n",
      "Running Accuracy is 46.875\n",
      "Running Loss is 0.694473018248876\n",
      "Running Accuracy is 49.47916666666667\n",
      "Running Loss is 0.6962308543069022\n",
      "Running Accuracy is 47.767857142857146\n",
      "Running Loss is 0.6930588930845261\n",
      "Running Accuracy is 49.21875\n",
      "Running Loss is 0.6926735440889994\n",
      "Running Accuracy is 49.65277777777778\n",
      "Running Loss is 0.6949877262115478\n",
      "Running Accuracy is 49.0625\n",
      "Running Loss is 0.6935983354395087\n",
      "Running Accuracy is 49.14772727272727\n",
      "Running Loss is 0.6957847227652868\n",
      "Running Accuracy is 47.91666666666667\n",
      "Running Loss is 0.6953497804128207\n",
      "Running Accuracy is 48.07692307692308\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch=\u001b[39m\u001b[38;5;124m\"\u001b[39m,e\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     epoch_loss,epoch_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss=\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Acc\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch_acc)\n\u001b[0;32m      6\u001b[0m     epoch_loss,epoch_acc\u001b[38;5;241m=\u001b[39meval_one_epoch()\n",
      "Cell \u001b[1;32mIn[41], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Accuracy is\u001b[39m\u001b[38;5;124m\"\u001b[39m,running_acc)\n\u001b[0;32m     19\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs=5\n",
    "for e in range(n_epochs):\n",
    "    print(\"Epoch=\",e+1, sep=\"\", end=\", \")\n",
    "    epoch_loss,epoch_acc=train_one_epoch()\n",
    "    print(\"Train Loss=\", epoch_loss, \"Train Acc\", epoch_acc)\n",
    "    epoch_loss,epoch_acc=eval_one_epoch()\n",
    "    print(\"Eval Loss=\", epoch_loss, \"Eval Acc\", epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
